<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>h4 Toolkit :: Cyberpunk Ops Dossier</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root {
      --bg-primary: #05060d;
      --bg-secondary: #111424;
      --bg-panel: #15192e;
      --text-primary: #f8f8ff;
      --text-secondary: #a9b2d6;
      --accent: #ffd300;
      --accent-muted: rgba(255, 211, 0, 0.45);
      --glow: 0 0 25px rgba(255, 211, 0, 0.6);
      --mono: "Fira Code", "Cascadia Code", "Source Code Pro", monospace;
      font-family: "Rajdhani", "Segoe UI", sans-serif;
    }

    body {
      background: linear-gradient(135deg, var(--bg-primary), #090c1d 45%, #06050d 100%);
      color: var(--text-primary);
      margin: 0;
      padding: 0;
      min-height: 100vh;
      display: flex;
      flex-direction: column;
    }

    header {
      padding: 3rem 4vw 1rem;
      position: relative;
      background: radial-gradient(circle at top left, rgba(255, 211, 0, 0.18), transparent 55%);
    }

    header h1 {
      font-size: clamp(2.5rem, 5vw, 3.8rem);
      letter-spacing: 0.18em;
      text-transform: uppercase;
      margin: 0 0 0.5rem;
      color: var(--accent);
      text-shadow: var(--glow);
    }

    header p {
      color: var(--text-secondary);
      max-width: 60ch;
      margin: 0;
      font-size: 1.1rem;
    }

    nav {
      background: rgba(21, 25, 46, 0.9);
      padding: 0.75rem 4vw;
      position: sticky;
      top: 0;
      z-index: 999;
      backdrop-filter: blur(6px);
      border-bottom: 1px solid var(--accent-muted);
    }

    nav ul {
      list-style: none;
      display: flex;
      gap: clamp(1rem, 2vw, 2.5rem);
      margin: 0;
      padding: 0;
      flex-wrap: wrap;
    }

    nav a {
      color: var(--text-secondary);
      text-decoration: none;
      font-weight: 600;
      position: relative;
      padding-bottom: 0.3rem;
    }

    nav a::after {
      content: "";
      position: absolute;
      left: 0;
      bottom: 0;
      width: 100%;
      height: 2px;
      background: var(--accent);
      transform: scaleX(0);
      transform-origin: left;
      transition: transform 0.25s ease;
      box-shadow: var(--glow);
    }

    nav a:hover::after,
    nav a:focus-visible::after {
      transform: scaleX(1);
    }

    main {
      padding: 2.5rem 4vw 4rem;
      display: grid;
      gap: 2.5rem;
    }

    section {
      background: var(--bg-panel);
      border: 1px solid rgba(255, 211, 0, 0.15);
      border-radius: 14px;
      padding: 2rem;
      position: relative;
      overflow: hidden;
    }

    section::before {
      content: "";
      position: absolute;
      inset: 0;
      background: linear-gradient(140deg, rgba(255, 211, 0, 0.05), transparent 60%);
      pointer-events: none;
    }

    section h2 {
      margin-top: 0;
      font-size: 1.8rem;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: var(--accent);
      text-shadow: 0 0 12px rgba(255, 211, 0, 0.45);
    }

    h3 {
      margin-top: 2rem;
      letter-spacing: 0.06em;
      text-transform: uppercase;
      color: var(--accent);
      font-size: 1.25rem;
    }

    p, li {
      color: var(--text-secondary);
      line-height: 1.65;
      max-width: 80ch;
    }

    strong {
      color: var(--accent);
    }

    .grid-columns {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
      gap: 1.5rem;
    }

    .panel {
      background: rgba(10, 13, 25, 0.6);
      border: 1px solid rgba(255, 211, 0, 0.25);
      border-radius: 12px;
      padding: 1.5rem;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
    }

    .tag {
      display: inline-block;
      padding: 0.2rem 0.55rem;
      border-radius: 999px;
      font-size: 0.8rem;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      border: 1px solid rgba(255, 211, 0, 0.6);
      margin-right: 0.5rem;
      color: var(--accent);
    }

    details {
      background: rgba(9, 12, 26, 0.7);
      border-left: 3px solid var(--accent);
      padding: 1rem 1.2rem;
      margin: 1.5rem 0;
      border-radius: 10px;
    }

    details summary {
      cursor: pointer;
      font-weight: 600;
      letter-spacing: 0.05em;
      color: var(--accent);
    }

    .reminder {
      border-left: 4px solid rgba(255, 211, 0, 0.8);
      padding-left: 1rem;
      margin: 1.5rem 0;
      font-style: italic;
      color: var(--text-secondary);
    }

    pre {
      background: rgba(5, 7, 16, 0.9);
      padding: 1.5rem;
      border-radius: 12px;
      overflow-x: auto;
      font-family: var(--mono);
      font-size: 0.85rem;
      color: #f8ffa7;
      border: 1px solid rgba(255, 211, 0, 0.25);
      box-shadow: inset 0 0 25px rgba(255, 211, 0, 0.08);
    }

    code {
      font-family: var(--mono);
    }

    footer {
      padding: 2rem 4vw 3rem;
      text-align: center;
      color: rgba(255, 211, 0, 0.7);
      font-size: 0.85rem;
      letter-spacing: 0.12em;
      text-transform: uppercase;
    }

    @media (max-width: 768px) {
      main {
        padding: 2rem 5vw 3rem;
      }
      section {
        padding: 1.5rem;
      }
      pre {
        font-size: 0.78rem;
      }
    }
  </style>
</head>
<body>
  <header>
    <h1>h4 Toolkit Operations Log</h1>
    <p>
      Cyberpunk diagnostics dossier detailing the build-out of the h4 Toolkit custom nodes for ComfyUI.
      Use this as the canonical reference for architecture, behaviour, and future-proofing tactics before shipping.
    </p>
  </header>

  <nav>
    <ul>
      <li><a href="#mission">Mission Recap</a></li>
      <li><a href="#architecture">Architecture &amp; Pipelines</a></li>
      <li><a href="#plot-node">Plot Node Deep Dive</a></li>
      <li><a href="#debug-node">Debug-a-tron Deep Dive</a></li>
      <li><a href="#dependencies">Dependencies &amp; Telemetry</a></li>
      <li><a href="#code">Code Appendix</a></li>
      <li><a href="#roadmap">Future-Proof Planning</a></li>
    </ul>
  </nav>

  <main>
    <section id="mission">
      <h2>Mission Recap</h2>
      <div class="grid-columns">
        <div class="panel">
          <span class="tag">Objective</span>
          <p>Deliver a polished, stability-focused node suite for ComfyUI that feels native while remaining hyper-observable during development.</p>
          <ul>
            <li><strong>Plot Node</strong> orchestrates checkpoint loading, sampler execution, LoRA layering, grid assembly, and fast previews.</li>
            <li><strong>Debug-a-tron 3000</strong> acts as an adaptive, console-first router capable of monitoring or forwarding every data type in a workflow.</li>
            <li><strong>UX polish</strong> via JavaScript injects placeholders, tooltips, and dynamic sockets without bloating the node count.</li>
          </ul>
        </div>
        <div class="panel">
          <span class="tag">Guiding Principles</span>
          <ul>
            <li>Ship production-ready code in a single pass: no placeholders, no silent branches, no half-written functions.</li>
            <li>Stay resource-light: defer heavy work until needed, reuse latents, and keep previews in low-cost lanes.</li>
            <li>Maintain clairvoyant visibility: every meaningful action emits colour-coded console traces.</li>
          </ul>
        </div>
      </div>
      <div class="reminder">
        Maintainer Marker &mdash; confirm <code>TOOLKIT_VERSION</code>, <code>PLOT_NODE_VERSION</code>, and <code>DEBUG_NODE_VERSION</code> stay aligned with release notes before packaging.
      </div>
    </section>

    <section id="architecture">
      <h2>Architecture &amp; Pipelines</h2>
      <h3>Core Files</h3>
      <ul>
        <li><strong><code>h4_Plot/nodes.py</code></strong> &mdash; houses reusable helpers, PlotNode, Debug-a-tron 3000, and the export dictionaries.</li>
        <li><strong><code>h4_Plot/__init__.py</code></strong> &mdash; verifies dependencies, announces updates, imports nodes, and prints the cybernetic status banner.</li>
        <li><strong><code>h4_Plot/js/h4_ui.js</code></strong> &mdash; injects UI niceties (placeholders, tooltips, dynamic sockets, orientation toggles).</li>
        <li><strong><code>h4_Plot/requirements.txt</code></strong> &mdash; minimal dependency manifest for manual installs.</li>
      </ul>

      <h3>Execution Flow</h3>
      <ol>
        <li><strong>Startup</strong> &mdash; <code>__init__.py</code> checks/installs dependencies, emits update notices, imports nodes, then renders the neon-lit banner.</li>
        <li><strong>User Interaction</strong> &mdash; UI script decorates widgets, spawns dynamic connectors, and keeps layout toggles responsive.</li>
        <li><strong>PlotNode Run</strong>
          <ul>
            <li>Axis descriptors parsed with knowledge of on-disk checkpoints/Loras.</li>
            <li>Generation plans computed (cartesian product across axes).</li>
            <li>Checkpoint + prompt encoding + LoRA patching orchestrated per plan.</li>
            <li>DPM++ 2M SDE + Karras sampler runs with rapid previews pushed via <code>latent_preview</code> when present.</li>
            <li>Decoded outputs assembled into square-first grids; structured JSON report returned.</li>
          </ul>
        </li>
        <li><strong>Debug-a-tron Run</strong> &mdash; inspects every inbound payload, logs metadata, and either mirrors or nulls outputs depending on mode.</li>
      </ol>

      <details>
        <summary>Telemetry Hooks</summary>
        <p>All helpers and node methods route through <code>ToolkitLogger</code>. Flip <code>VERBOSE_LOGGING</code> to silence console noise post-debug. Preview failures are downgraded to warnings to avoid collapsing the run.</p>
      </details>

      <div class="reminder">
        Maintainer Marker &mdash; when adding new nodes, respect the “three-node ceiling” mantra and expand <code>NODE_CLASS_MAPPINGS</code> only with explicit user consent.
      </div>
    </section>

    <section id="plot-node">
      <h2>Plot Node Deep Dive</h2>
      <p>The Plot Node is a self-contained composer that can operate standalone or accept mid-pipeline overrides. Highlights:</p>
      <ul>
        <li><strong>Inputs</strong>: seed, steps, CFG, dimensions, base checkpoint, prompts, axis definitions, and optional model/clip/vae/conditioning/latent/image overrides.</li>
        <li><strong>Axis Language</strong>: lines prefixed with <code>checkpoint:</code>, <code>lora:</code>, or plain text prompt modifiers; synonyms (<code>none</code>, <code>base</code>, <code>default</code>) map to the identity descriptor.</li>
        <li><strong>Sampler Stack</strong>: DPM++ 2M SDE sampler with Karras scheduler, seeded per-plan (seed + index) to keep comparisons deterministic.</li>
        <li><strong>Latent Handling</strong>: clones input latent/image-encoded latent to avoid destructive mutations; empties GPU cache between runs when CUDA is available.</li>
        <li><strong>Outputs</strong>: composite grid image, last latent state, execution report (JSON payload with plan/result pairs).</li>
      </ul>

      <h3>Observability</h3>
      <p>Every stage logs to console (load checkpoint, encode prompts, apply LoRAs, preview steps). The execution report can be parsed downstream for analytics or displayed in the Debug-a-tron.</p>

      <div class="reminder">
        Maintainer Marker &mdash; if swapping sampler defaults, adjust both the Python <code>sampler_name</code>/<code>scheduler</code> constants and the documentation callouts below to keep parity.</div>
    </section>

    <section id="debug-node">
      <h2>Debug-a-tron 3000 Deep Dive</h2>
      <p>The Debug-a-tron is built as an inline router that can “sniff” or “forward” any data stream without rerouting your graph.</p>
      <ul>
        <li><strong>Modes</strong>: <em>Monitor</em> nulls all outputs but keeps verbose logging active; <em>Passthrough</em> forwards each payload untouched.</li>
        <li><strong>Orientation</strong>: UI toggle flips between horizontal (wide) and vertical (stacked) layouts to keep spaghetti at bay.</li>
        <li><strong>Dynamic Sockets</strong>: eight wildcard lanes auto-type themselves when wires attach, then spawn the next blank socket.</li>
        <li><strong>Log Payload</strong>: returns a JSON string listing every inbound tensor/object with shape/dtype metadata for auditing.</li>
      </ul>

      <h3>Inline Diagnostics</h3>
      <p>Use the node inline with complex graphs to watch latents, conditioning, models, and text prompts simultaneously. Logs reflect connection type, device, and other salient metadata without blocking execution.</p>

      <div class="reminder">
        Maintainer Marker &mdash; if ComfyUI introduces new socket types, extend the wildcard count and UI type dictionary in tandem.</div>
    </section>

    <section id="dependencies">
      <h2>Dependencies &amp; Telemetry</h2>
      <ul>
        <li><strong>Auto-install</strong>: <code>__init__.py</code> ensures <code>colorama</code>, <code>numpy</code>, and <code>torch</code> exist; installs missing packages with error reporting.</li>
        <li><strong>Version Alerts</strong>: compares installed versions to PyPI; prints cyan notices when updates are available without auto-upgrading.</li>
        <li><strong>Status Banner</strong>: neon table shows loaded/failed nodes with checkmarks or crosses; perfect for Stability Matrix logs.</li>
        <li><strong>Web Assets</strong>: <code>WEB_DIRECTORY</code> exported so ComfyUI loads <code>js/h4_ui.js</code> automatically.</li>
      </ul>

      <details>
        <summary>Deployment Notes</summary>
        <ul>
          <li>Set <code>VERBOSE_LOGGING = False</code> before public release to quiet console spam.</li>
          <li>Maintain <code>requirements.txt</code> in lockstep with <code>DEPENDENCIES</code> to keep manual installs frictionless.</li>
        </ul>
      </details>

      <div class="reminder">
        Maintainer Marker &mdash; plan a VS-aware configuration so the banner can display the current ComfyUI/Stability Matrix version once the upstream API exposes it.</div>
    </section>

    <section id="code">
      <h2>Code Appendix</h2>
      <p>The following mirrors the exact source currently in play. Update this dossier whenever the code shifts to keep the documentation in sync.</p>

      <h3>nodes.py</h3>
      <pre id="nodes-code" aria-label="nodes.py source"></pre>

      <h3>__init__.py</h3>
      <pre id="init-code" aria-label="__init__.py source"></pre>

      <div class="reminder">
        Maintainer Marker &mdash; after every refactor, re-export the latest code snippets using the helper script planned in the future-proof section.</div>
    </section>

    <section id="roadmap">
      <h2>Future-Proof Planning</h2>
      <h3>Immediate Enhancements</h3>
      <ul>
        <li>Add UI toggle to switch between verbose and quiet modes without editing Python.</li>
        <li>Introduce optional persistence for Debug-a-tron socket configurations (per workflow).</li>
        <li>Expose grid metadata (rows/cols, descriptors) as a second JSON output for downstream automation.</li>
      </ul>

      <h3>Version Awareness Initiative</h3>
      <ul>
        <li>Augment <code>__init__.py</code> to detect the running ComfyUI/Stability Matrix version and display it in the banner.</li>
        <li>Embed toolkit schema versioning so saved workflows can detect incompatibilities.</li>
        <li>Include semantic version metadata in <code>execution_report</code> and Debug-a-tron logs for audit trails.</li>
      </ul>

      <h3>Maintenance Rituals</h3>
      <ul>
        <li>Run quarterly dependency health checks; document recommended versions alongside the auto-installer.</li>
        <li>Automate regeneration of this HTML dossier whenever the Python modules change (planned helper script should pull latest code and rehydrate the <code>&lt;pre&gt;</code> blocks).</li>
        <li>Archive major releases in a <code>docs/history</code> ledger for regression comparisons.</li>
      </ul>
    </section>
  </main>

  <footer>
    h4 Toolkit &mdash; documented for the future shapers.
  </footer>

  <script>
    const nodesSource = `# -*- coding: utf-8 -*-\n"""h4 Toolkit custom nodes for ComfyUI.\n\nThis module implements the Plot node (end-to-end sampler + grid generator with\nprogress previews) and the Debug-a-tron-3000 adaptive router/inspector. Both\nnodes are instrumented with extremely verbose logging so that every action is\nvisible in the Stability Matrix / ComfyUI console during development.\n\nThe implementation leans on existing ComfyUI building blocks (samplers, VAE\nutilities, CLIP encoders, etc.) to ensure forward compatibility while keeping\nresource usage as low as practical. All helper utilities in this file are kept\nlightweight and are re-used by both nodes to honour the "three node" policy.\n"""\n\nfrom __future__ import annotations\n\nimport functools\nimport json\nimport math\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, Iterable, List, Optional, Tuple\n\nimport numpy as np\nimport torch\n\nfrom colorama import Fore, Style\n\ntry:  # pragma: no cover - optional preview module\n    import latent_preview\nexcept ImportError:  # pragma: no cover - preview is optional\n    latent_preview = None\n\nimport comfy.model_management as model_management\nimport comfy.samplers as comfy_samplers\nimport comfy.sd as comfy_sd\nimport comfy.utils as comfy_utils\nimport folder_paths\nimport nodes as comfy_nodes\n\nTOOLKIT_VERSION = "1.0.0"\nPLOT_NODE_VERSION = "1.2.0"\nDEBUG_NODE_VERSION = "2.0.0"\n\nPLOT_WIDGET_TOOLTIPS: Dict[str, str] = {\n    "seed": "Random generator seed. Use the same value to make fair comparisons across runs.",\n    "steps": "Number of denoising steps. Higher values can add detail at the cost of runtime.",\n    "cfg": "Classifier free guidance strength. Set to 1 to ignore the external negative prompt.",\n    "width": "Output width in pixels. Stay close to the base model's native resolution to avoid artefacts.",\n    "height": "Output height in pixels. Match width for square generations or adjust to taste.",\n    "checkpoint": "Primary checkpoint to load when no axis entry overrides it.",\n    "positive_prompt": "Text that describes what you want to see. Plain English encouraged.",\n    "negative_prompt": "Terms that should be avoided. Ignored automatically when CFG is 1.",\n    "x_axis_values": "One modifier per line. Prefix with checkpoint:, lora:, or leave plain text to extend the prompt.",\n    "y_axis_values": "Same syntax as the X axis. The cartesian product builds your comparison grid.",\n}\n\nDEBUG_WIDGET_TOOLTIPS: Dict[str, str] = {\n    "mode": "Monitor captures signals without forwarding. Passthrough logs and forwards everything.",\n    "orientation": "Choose Horizontal to lay sockets left-to-right or Vertical to stack them top-to-bottom.",\n}\n\n# Verbosity flag lives at module level so we can quickly disable the firehose.\nVERBOSE_LOGGING = True\n\n\nclass ToolkitLogger:\n    """Structured, colourised logger for development-time visibility."""\n\n    def __init__(self, component: str) -> None:\n        self.component = component\n        self._start_ts = time.time()\n\n    def _emit(self, level: str, colour: str, message: str) -> None:\n        if not VERBOSE_LOGGING:\n            return\n        elapsed = time.time() - self._start_ts\n        prefix = f"[{elapsed:8.3f}s][{self.component}][{level}]"\n        print(f"{colour}{prefix} {message}{Style.RESET_ALL}")\n\n    def trace(self, message: str) -> None:\n        self._emit("TRACE", Fore.MAGENTA, message)\n\n    def info(self, message: str) -> None:\n        self._emit("INFO", Fore.CYAN, message)\n\n    def warn(self, message: str) -> None:\n        self._emit("WARN", Fore.YELLOW, message)\n\n    def error(self, message: str) -> None:\n        self._emit("ERROR", Fore.RED, message)\n\n\nGLOBAL_LOGGER = ToolkitLogger("h4_ToolKit")\n\n\n@dataclass(frozen=True)\nclass AxisDescriptor:\n    """Represents a user-specified modifier from the X/Y axis panels."""\n\n    source_label: str\n    kind: str  # checkpoint | lora | prompt | identity\n    name: Optional[str] = None\n    strength: float = 1.0\n    prompt_suffix: str = ""\n\n    def describe(self) -> str:\n        if self.kind == "checkpoint" and self.name:\n            return f"checkpoint={self.name}"\n        if self.kind == "lora" and self.name:\n            return f"lora={self.name}@{self.strength:.2f}"\n        if self.kind == "prompt" and self.prompt_suffix:\n            return f"prompt+={self.prompt_suffix}"\n        return "identity"\n\n\n@dataclass\nclass GenerationPlan:\n    """Concrete execution plan for a single plot cell."""\n\n    label: str\n    checkpoint_name: Optional[str]\n    prompt_suffix: str\n    loras: List[Tuple[str, float]] = field(default_factory=list)\n\n    def as_dict(self) -> Dict[str, Any]:\n        return {\n            "label": self.label,\n            "checkpoint": self.checkpoint_name,\n            "prompt_suffix": self.prompt_suffix,\n            "loras": [\n                {"name": name, "strength": strength} for name, strength in self.loras\n            ],\n        }\n\n\ndef _split_lines(value: str) -> List[str]:\n    return [line.strip() for line in value.splitlines() if line.strip()]\n\n\ndef parse_axis_entries(raw_text: str) -> List[AxisDescriptor]:\n    """Parses axis definitions into typed descriptors."""\n\n    entries: List[AxisDescriptor] = []\n    if raw_text.strip():\n        GLOBAL_LOGGER.trace(\n            f"Parsing axis entries from text ({len(raw_text.splitlines())} lines)"\n        )\n    checkpoints = set(folder_paths.get_filename_list("checkpoints"))\n    loras = set(folder_paths.get_filename_list("loras"))\n    for raw_line in _split_lines(raw_text):\n        lowered = raw_line.lower()\n        if lowered in ("none", "base", "default"):\n            entries.append(AxisDescriptor(raw_line, "identity"))\n            continue\n        if lowered.startswith("checkpoint:"):\n            name = raw_line.split(":", 1)[1].strip()\n            if not name:\n                continue\n            entries.append(AxisDescriptor(raw_line, "checkpoint", name=name))\n            continue\n        if lowered.startswith("lora:"):\n            remainder = raw_line.split(":", 1)[1].strip()\n            if "@" in remainder:\n                name_part, strength_part = remainder.split("@", 1)\n            elif "|" in remainder:\n                name_part, strength_part = remainder.split("|", 1)\n            else:\n                name_part, strength_part = remainder, "1.0"\n            name = name_part.strip()\n            strength = float(strength_part.strip()) if strength_part.strip() else 1.0\n            entries.append(AxisDescriptor(raw_line, "lora", name=name, strength=strength))\n            continue\n        if raw_line in checkpoints:\n            entries.append(AxisDescriptor(raw_line, "checkpoint", name=raw_line))\n            continue\n        if raw_line in loras:\n            entries.append(AxisDescriptor(raw_line, "lora", name=raw_line, strength=1.0))\n            continue\n        entries.append(AxisDescriptor(raw_line, "prompt", prompt_suffix=raw_line))\n    return entries\n\n\ndef build_generation_matrix(\n    base_checkpoint: Optional[str],\n    x_descriptors: List[AxisDescriptor],\n    y_descriptors: List[AxisDescriptor],\n) -> List[GenerationPlan]:\n    """Computes the cartesian product of axis descriptors into concrete plans."""\n\n    def expand(axis_desc: List[AxisDescriptor]) -> List[List[AxisDescriptor]]:\n        return [[desc] for desc in axis_desc] or [[AxisDescriptor("∅", "identity")]]\n\n    runs: List[GenerationPlan] = []\n    for x_bundle in expand(x_descriptors):\n        for y_bundle in expand(y_descriptors):\n            combined = x_bundle + y_bundle\n            checkpoint_name = base_checkpoint\n            suffixes: List[str] = []\n            loras: List[Tuple[str, float]] = []\n            label_parts: List[str] = []\n            for descriptor in combined:\n                label_parts.append(descriptor.describe())\n                if descriptor.kind == "checkpoint" and descriptor.name:\n                    checkpoint_name = descriptor.name\n                elif descriptor.kind == "lora" and descriptor.name:\n                    loras.append((descriptor.name, descriptor.strength))\n                elif descriptor.kind == "prompt" and descriptor.prompt_suffix:\n                    suffixes.append(descriptor.prompt_suffix)\n            label = " | ".join([part for part in label_parts if part != "identity"]) or "base"\n            plan = GenerationPlan(\n                label=label,\n                checkpoint_name=checkpoint_name,\n                prompt_suffix="\n".join(suffixes) if suffixes else "",\n                loras=loras,\n            )\n            runs.append(plan)\n            GLOBAL_LOGGER.trace(\n                f"Planned run: checkpoint={plan.checkpoint_name}, prompt+={plan.prompt_suffix!r}, loras={plan.loras}"\n            )\n    return runs\n\n\ndef auto_square_layout(total_images: int) -> Tuple[int, int]:\n    """Find a grid layout that is as square as possible."""\n\n    if total_images <= 0:\n        return (1, 1)\n    root = int(math.sqrt(total_images))\n    for columns in range(root, 0, -1):\n        if total_images % columns == 0:\n            rows = total_images // columns\n            return (max(1, rows), max(1, columns))\n    columns = max(1, root)\n    rows = math.ceil(total_images / columns)\n    return (rows, columns)\n\n\ndef compose_image_grid(images: List[torch.Tensor]) -> torch.Tensor:\n    """Stack a list of decoded image tensors into a single grid image."""\n\n    if not images:\n        raise ValueError("compose_image_grid called with no images")\n    batched: List[torch.Tensor] = []\n    for tensor in images:\n        if tensor.ndim == 4:\n            for item in tensor:\n                batched.append(item)\n        else:\n            batched.append(tensor)\n    total = len(batched)\n    rows, cols = auto_square_layout(total)\n    GLOBAL_LOGGER.trace(\n        f"Composing grid with {total} tiles -> layout {rows}x{cols}"\n    )\n    height = batched[0].shape[1]\n    width = batched[0].shape[2]\n    channels = batched[0].shape[0]\n    grid = torch.zeros((channels, rows * height, cols * width), dtype=batched[0].dtype, device=batched[0].device)\n    for idx, tile in enumerate(batched):\n        r = idx // cols\n        c = idx % cols\n        grid[:, r * height : (r + 1) * height, c * width : (c + 1) * width] = tile\n    return grid.unsqueeze(0)\n\n\ndef clone_latent(latent: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    return {key: value.clone() if torch.is_tensor(value) else value for key, value in latent.items()}\n\n\nclass PlotNode:\n    """All-in-one checkpoint loader, sampler, scheduler, and grid plotter."""\n\n    @classmethod\n    def INPUT_TYPES(cls) -> Dict[str, Any]:  # noqa: N802 - ComfyUI naming convention\n        checkpoint_list = folder_paths.get_filename_list("checkpoints")\n        default_checkpoint = checkpoint_list[0] if checkpoint_list else ""\n        return {\n            "required": {\n                "seed": ("INT", {"default": 0, "min": 0, "max": 2**32 - 1}),\n                "steps": ("INT", {"default": 20, "min": 1, "max": 150}),\n                "cfg": ("FLOAT", {"default": 7.5, "min": 1.0, "max": 30.0, "step": 0.1}),\n                "width": ("INT", {"default": 512, "min": 64, "max": 2048, "step": 64}),\n                "height": ("INT", {"default": 512, "min": 64, "max": 2048, "step": 64}),\n                "checkpoint": (checkpoint_list, {"default": default_checkpoint}),\n                "positive_prompt": (\n                    "STRING",\n                    {\n                        "default": "",\n                        "multiline": True,\n                    },\n                ),\n                "negative_prompt": (\n                    "STRING",\n                    {\n                        \"default\": \"\",\n                        \"multiline\": True,\n                    },\n                ),\n                \"x_axis_values\": (\n                    \"STRING\",\n                    {\n                        \"default\": \"\",\n                        \"multiline\": True,\n                    },\n                ),\n                \"y_axis_values\": (\n                    \"STRING\",\n                    {\n                        \"default\": \"\",\n                        \"multiline\": True,\n                    },\n                ),\n            },\n            \"optional\": {\n                \"model_in\": (\"MODEL\",),\n                \"clip_in\": (\"CLIP\",),\n                \"vae_in\": (\"VAE\",),\n                \"conditioning_in\": (\"CONDITIONING\",),\n                \"latent_in\": (\"LATENT\",),\n                \"image_in\": (\"IMAGE\",),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\", \"LATENT\", \"STRING\")\n    RETURN_NAMES = (\"grid_image\", \"last_latent\", \"execution_report\")\n    FUNCTION = \"run_pipeline\"\n    CATEGORY = \"h4 Toolkit/Generation\"\n\n    def __init__(self) -> None:\n        self.logger = ToolkitLogger(\"PlotNode\")\n        self.clip_encoder = comfy_nodes.CLIPTextEncode()\n        self.vae_decode = comfy_nodes.VAEDecode()\n        self.vae_encode = comfy_nodes.VAELatentEncode()\n        self.empty_latent = comfy_nodes.EmptyLatentImage()\n        self.lora_loader = comfy_nodes.LoraLoader()\n\n    def _load_checkpoint(self, checkpoint_name: str) -> Tuple[Any, Any, Any]:\n        self.logger.trace(f\"Loading checkpoint: {checkpoint_name}\")\n        ckpt_path = folder_paths.get_full_path(\"checkpoints\", checkpoint_name)\n        model, clip, vae, _ = comfy_sd.load_checkpoint_guess_config(\n            ckpt_path, output_vae=True, output_clip=True\n        )\n        self.logger.info(f\"Checkpoint loaded: {checkpoint_name}\")\n        return model, clip, vae\n\n    def _encode_prompt(self, clip: Any, prompt: str) -> Any:\n        self.logger.trace(f\"Encoding prompt: {prompt[:60]}...\")\n        result = self.clip_encoder.encode(clip, prompt)\n        return result[0] if isinstance(result, tuple) else result\n\n    def _prepare_latent(\n        self,\n        vae: Any,\n        width: int,\n        height: int,\n        latent_in: Optional[Dict[str, torch.Tensor]],\n        image_in: Optional[torch.Tensor],\n    ) -> Dict[str, torch.Tensor]:\n        if latent_in is not None:\n            self.logger.info(\"Using supplied latent as base\")\n            return clone_latent(latent_in)\n        if image_in is not None:\n            self.logger.info(\"Encoding supplied image into latent\")\n            latent = self.vae_encode.encode(vae, image_in)[0]\n            return clone_latent(latent)\n        self.logger.info(\"Creating empty latent with requested dimensions\")\n        latent = self.empty_latent.generate(width, height, 1)\n        return clone_latent(latent)\n\n    def _apply_loras(\n        self,\n        model: Any,\n        clip: Any,\n        loras: Iterable[Tuple[str, float]],\n    ) -> Tuple[Any, Any]:\n        patched_model, patched_clip = model, clip\n        for lora_name, strength in loras:\n            self.logger.trace(f\"Applying LoRA {lora_name} @ {strength}\")\n            patched_model, patched_clip = self.lora_loader.load_lora(\n                patched_model, patched_clip, lora_name, strength, strength\n            )\n        return patched_model, patched_clip\n\n    def _run_sampler(\n        self,\n        model: Any,\n        vae: Any,\n        positive: Any,\n        negative: Any,\n        latent: Dict[str, torch.Tensor],\n        seed: int,\n        steps: int,\n        cfg: float,\n        denoise: float,\n    ) -> Dict[str, torch.Tensor]:\n        samples = latent[\"samples\"]\n        generator = torch.Generator(device=samples.device).manual_seed(int(seed))\n        noise = torch.randn_like(samples, generator=generator)\n        sampler_name = \"dpmpp_2m_sde\"\n        scheduler = \"karras\"\n        preview_token = f\"plot-preview-{time.time():.0f}-{seed}\"\n\n        def callback(step: int, x0: torch.Tensor, *_args: Any) -> None:\n            self.logger.trace(f\"Preview step {step}/{steps}\")\n            if latent_preview is None:\n                return\n            try:\n                decoded = latent_preview.decode_latent_preview(vae, x0)\n                latent_preview.publish_preview(decoded, preview_token)\n            except Exception as exc:  # pragma: no cover - best effort only\n                self.logger.warn(f\"Preview publishing failed: {exc}\")\n\n        self.logger.info(\n            f\"Sampling with {sampler_name} + {scheduler}, steps={steps}, cfg={cfg:.2f}, denoise={denoise:.2f}\"\n        )\n        result = comfy_samplers.sample(\n            model=model,\n            noise=noise,\n            steps=steps,\n            cfg=cfg,\n            sampler_name=sampler_name,\n            scheduler=scheduler,\n            positive=positive,\n            negative=negative,\n            latent=latent,\n            seed=seed,\n            denoise=denoise,\n            disable_noise=False,\n            start_step=0,\n            last_step=-1,\n            force_full_denoise=False,\n            callback=callback,\n        )\n        return result\n\n    def _decode(self, vae: Any, latent: Dict[str, torch.Tensor]) -> torch.Tensor:\n        images = self.vae_decode.decode(vae, latent)[0]\n        self.logger.trace("Decoded latent into image tensor")\n        return images\n\n    def _build_report(\n        self,\n        plans: List[GenerationPlan],\n        results: List[Dict[str, Any]],\n    ) -> str:\n        payload = {\n            "node": "PlotNode",\n            "version": PLOT_NODE_VERSION,\n            "runs": [\n                {\n                    "plan": plan.as_dict(),\n                    "result": summary,\n                }\n                for plan, summary in zip(plans, results)\n            ],\n        }\n        report = json.dumps(payload, indent=2)\n        self.logger.info("Execution report generated")\n        return report\n\n    def run_pipeline(\n        self,\n        seed: int,\n        steps: int,\n        cfg: float,\n        width: int,\n        height: int,\n        checkpoint: str,\n        positive_prompt: str,\n        negative_prompt: str,\n        x_axis_values: str,\n        y_axis_values: str,\n        model_in: Optional[Any] = None,\n        clip_in: Optional[Any] = None,\n        vae_in: Optional[Any] = None,\n        conditioning_in: Optional[Any] = None,\n        latent_in: Optional[Dict[str, torch.Tensor]] = None,\n        image_in: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], str]:\n        self.logger.info("Plot pipeline starting")\n        model_cache: Dict[str, Tuple[Any, Any, Any]] = {}\n\n        def get_model_bundle(name: Optional[str]) -> Tuple[Any, Any, Any]:\n            if model_in is not None and name is None:\n                return model_in, clip_in, vae_in\n            lookup_name = name or checkpoint\n            if lookup_name not in model_cache:\n                model_cache[lookup_name] = self._load_checkpoint(lookup_name)\n            return model_cache[lookup_name]\n\n        plans = build_generation_matrix(\n            base_checkpoint=checkpoint,\n            x_descriptors=parse_axis_entries(x_axis_values),\n            y_descriptors=parse_axis_entries(y_axis_values),\n        )\n        self.logger.info(f"Generated {len(plans)} execution plan(s)")\n\n        base_model, base_clip, base_vae = get_model_bundle(None)\n        latent_base = self._prepare_latent(base_vae, width, height, latent_in, image_in)\n        negative = (\n            conditioning_in\n            if (conditioning_in is not None and cfg > 1.0)\n            else (self._encode_prompt(base_clip, negative_prompt) if cfg > 1.0 else self._encode_prompt(base_clip, ""))\n        )\n\n        grid_images: List[torch.Tensor] = []\n        last_latent: Dict[str, torch.Tensor] = latent_base\n        run_summaries: List[Dict[str, Any]] = []\n\n        for index, plan in enumerate(plans):\n            self.logger.info(f"Executing plan {index + 1}/{len(plans)} :: {plan.label}")\n            model_bundle = get_model_bundle(plan.checkpoint_name)\n            active_model, active_clip, active_vae = model_bundle\n            if conditioning_in is not None:\n                positive = conditioning_in\n            else:\n                effective_prompt = positive_prompt\n                if plan.prompt_suffix:\n                    effective_prompt = f"{positive_prompt}\n{plan.prompt_suffix}".strip()\n                positive = self._encode_prompt(active_clip, effective_prompt)\n            patched_model, patched_clip = self._apply_loras(active_model, active_clip, plan.loras)\n            latent_copy = clone_latent(latent_base)\n            result_latent = self._run_sampler(\n                model=patched_model,\n                vae=active_vae,\n                positive=positive,\n                negative=negative,\n                latent=latent_copy,\n                seed=seed + index,\n                steps=steps,\n                cfg=cfg,\n                denoise=1.0,\n            )\n            decoded = self._decode(active_vae, result_latent)\n            grid_images.append(decoded)\n            last_latent = result_latent\n            run_summaries.append(\n                {\n                    "label": plan.label,\n                    "seed": seed + index,\n                    "steps": steps,\n                    "cfg": cfg,\n                    "loras": plan.loras,\n                    "checkpoint": plan.checkpoint_name,\n                }\n            )\n            if torch.cuda.is_available():  # pragma: no cover - device-specific\n                torch.cuda.empty_cache()\n        model_management.soft_empty_cache()\n\n        grid = compose_image_grid(grid_images)\n        report = self._build_report(plans, run_summaries)\n        self.logger.info("Plot pipeline complete")\n        return grid, last_latent, report\n\n\nclass DebugATron3000:\n    """Adaptive router that mirrors signals per data type while logging everything."""\n\n    @classmethod\n    def INPUT_TYPES(cls) -> Dict[str, Any]:  # noqa: N802\n        return {\n            "required": {\n                "mode": (\n                    ["monitor", "passthrough"],\n                    {"default": "monitor"},\n                ),\n                "orientation": (\n                    ["horizontal", "vertical"],\n                    {"default": "horizontal"},\n                ),\n            },\n            "optional": {\n                "input_image": ("IMAGE",),\n                "input_latent": ("LATENT",),\n                "input_mask": ("MASK",),\n                "input_conditioning": ("CONDITIONING",),\n                "input_model": ("MODEL",),\n                "input_clip": ("CLIP",),\n                "input_vae": ("VAE",),\n                "input_tensor": ("TENSOR",),\n                "input_string": ("STRING",),\n                "any_slot_0": ("*",),\n                "any_slot_1": ("*",),\n                "any_slot_2": ("*",),\n                "any_slot_3": ("*",),\n                "any_slot_4": ("*",),\n                "any_slot_5": ("*",),\n                "any_slot_6": ("*",),\n                "any_slot_7": ("*",),\n            },\n        }\n\n    RETURN_TYPES = (\n        "IMAGE",\n        "LATENT",\n        "MASK",\n        "CONDITIONING",\n        "MODEL",\n        "CLIP",\n        "VAE",\n        "TENSOR",\n        "STRING",\n        "*",\n        "*",\n        "*",\n        "*",\n        "*",\n        "*",\n        "*",\n        "*",\n        "STRING",\n    )\n    RETURN_NAMES = (\n        "image_out",\n        "latent_out",\n        "mask_out",\n        "conditioning_out",\n        "model_out",\n        "clip_out",\n        "vae_out",\n        "tensor_out",\n        "string_out",\n        "any_out_0",\n        "any_out_1",\n        "any_out_2",\n        "any_out_3",\n        "any_out_4",\n        "any_out_5",\n        "any_out_6",\n        "any_out_7",\n        "debug_log",\n    )\n    FUNCTION = "route"\n    CATEGORY = "h4 Toolkit/Debug"\n\n    def __init__(self) -> None:\n        self.logger = ToolkitLogger("DebugATron3000")\n\n    def _summarise_tensor(self, tensor: torch.Tensor) -> str:\n        if tensor is None:\n            return "<none>"\n        return (\n            f"shape={tuple(tensor.shape)} dtype={tensor.dtype} device={tensor.device}"\n        )\n\n    def _summarise_object(self, obj: Any) -> str:\n        if obj is None:\n            return "<none>"\n        if torch.is_tensor(obj):\n            return self._summarise_tensor(obj)\n        return f"{type(obj).__name__}"\n\n    def route(\n        self,\n        mode: str,\n        orientation: str,\n        input_image: Optional[torch.Tensor] = None,\n        input_latent: Optional[Dict[str, torch.Tensor]] = None,\n        input_mask: Optional[torch.Tensor] = None,\n        input_conditioning: Optional[Any] = None,\n        input_model: Optional[Any] = None,\n        input_clip: Optional[Any] = None,\n        input_vae: Optional[Any] = None,\n        input_tensor: Optional[torch.Tensor] = None,\n        input_string: Optional[str] = None,\n        **dynamic_slots: Any,\n    ) -> Tuple[Any, ...]:\n        self.logger.info(\n            f"Debug router engaged :: mode={mode}, orientation={orientation}"\n        )\n        log_entries = []\n        payload_map = {\n            "IMAGE": input_image,\n            "LATENT": input_latent,\n            "MASK": input_mask,\n            "CONDITIONING": input_conditioning,\n            "MODEL": input_model,\n            "CLIP": input_clip,\n            "VAE": input_vae,\n            "TENSOR": input_tensor,\n            "STRING": input_string,\n        }\n        for kind, value in payload_map.items():\n            summary = self._summarise_object(value)\n            log_entries.append(f"{kind}: {summary}")\n            self.logger.trace(f"{kind} -> {summary}")\n        for slot_name, slot_value in sorted(dynamic_slots.items()):\n            descriptor = self._summarise_object(slot_value)\n            log_entries.append(f"{slot_name}: {descriptor}")\n            self.logger.trace(f"{slot_name} -> {descriptor}")\n        debug_payload = json.dumps(\n            {\n                "node": "DebugATron3000",\n                "version": DEBUG_NODE_VERSION,\n                "mode": mode,\n                "orientation": orientation,\n                "signals": log_entries,\n            },\n            indent=2,\n        )\n        if mode == "monitor":\n            passthroughs = (None,) * 9\n            dynamic_passthroughs = (None,) * 8\n        else:\n            passthroughs = (\n                input_image,\n                input_latent,\n                input_mask,\n                input_conditioning,\n                input_model,\n                input_clip,\n                input_vae,\n                input_tensor,\n                input_string,\n            )\n            dynamic_passthroughs = tuple(dynamic_slots.get(f"any_slot_{idx}") for idx in range(8))\n        self.logger.info("Debug routing complete")\n        return (*passthroughs, *dynamic_passthroughs, debug_payload)\n\n\nNODE_CLASS_MAPPINGS = {\n    "h4PlotNode": PlotNode,\n    "h4DebugATron3000": DebugATron3000,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    "h4PlotNode": f"h4 Plot (v{PLOT_NODE_VERSION})",\n    "h4DebugATron3000": f"Debug-a-tron 3000 (v{DEBUG_NODE_VERSION})",\n}\n\nNODE_TOOLTIP_MAPPINGS = {\n    "h4PlotNode": PLOT_WIDGET_TOOLTIPS,\n    "h4DebugATron3000": DEBUG_WIDGET_TOOLTIPS,\n}\n\n__all__ = [\n    "PlotNode",\n    "DebugATron3000",\n    "NODE_CLASS_MAPPINGS",\n    "NODE_DISPLAY_NAME_MAPPINGS",\n    "NODE_TOOLTIP_MAPPINGS",\n]\n`;

    const initSource = `# -*- coding: utf-8 -*-\n"""Entry point for the h4 Toolkit custom nodes package.\n\nResponsibilities handled here:\n- Dependency validation and on-demand installation for Python packages.\n- Optional update notification (no automatic upgrades).\n- Import of the node classes with defensive error handling.\n- Colourised startup table so toolkit status is front-and-centre in the console.\n"""\n\nfrom __future__ import annotations\n\nimport importlib\nimport importlib.util\nimport json\nimport subprocess\nimport sys\nimport urllib.error\nimport urllib.request\nfrom dataclasses import dataclass\nfrom importlib import metadata\nfrom typing import Dict, List\n\ntry:\n    from colorama import Fore, Style, init as colorama_init\nexcept ImportError:  # pragma: no cover - bootstrap path\n    class _BlankColour:  # type: ignore\n        RESET_ALL = ""\n\n        def __getattr__(self, _name: str) -> str:\n            return ""\n\n    Fore = Style = _BlankColour()  # type: ignore\n\n    def colorama_init(*_args, **_kwargs):  # type: ignore\n        return None\n\ncolorama_init(autoreset=True)\n\nDEPENDENCIES: Dict[str, str] = {\n    "colorama": "0.4.6",\n    "numpy": "1.24.0",\n    "torch": "1.13.0",\n}\n\nTOOLKIT_NAME = "h4_ToolKit"\nTOOLKIT_VERSION = "1.0.0"\n\n\ndef _emit(message: str, colour: str = Fore.LIGHTWHITE_EX) -> None:\n    print(f"{colour}[{TOOLKIT_NAME}] {message}{Style.RESET_ALL}")\n\n\ndef _ensure_dependency(name: str, minimum_version: str) -> None:\n    spec = importlib.util.find_spec(name)\n    if spec is None:\n        _emit(f"Installing missing dependency: {name}", Fore.YELLOW)\n        try:\n            subprocess.run(\n                [sys.executable, "-m", "pip", "install", f"{name}>={minimum_version}"],\n                check=True,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n            )\n        except subprocess.CalledProcessError as exc:  # pragma: no cover - installation failures logged\n            _emit(f"Failed to install {name}: {exc.stderr.decode(errors='ignore')}", Fore.RED)\n            return\n    else:\n        installed_version = metadata.version(name)\n        if _needs_update(installed_version, minimum_version):\n            _emit(\n                f"Dependency {name} is below recommended version {minimum_version} (installed {installed_version}).",\n                Fore.YELLOW,\n            )\n    _notify_if_newer(name)\n\n\ndef _needs_update(installed: str, minimum: str) -> bool:\n    return _normalise_version(installed) < _normalise_version(minimum)\n\n\ndef _normalise_version(version: str) -> List[int]:\n    parts = []\n    for fragment in version.replace("-", ".").split('.'):\n        if fragment.isdigit():\n            parts.append(int(fragment))\n        else:\n            parts.append(0)\n    while len(parts) < 3:\n        parts.append(0)\n    return parts[:3]\n\n\ndef _notify_if_newer(package_name: str) -> None:\n    try:\n        with urllib.request.urlopen(f"https://pypi.org/pypi/{package_name}/json", timeout=2.0) as response:\n            payload = json.loads(response.read().decode("utf-8"))\n            latest_version = payload["info"]["version"]\n    except (urllib.error.URLError, KeyError, TimeoutError, ValueError):  # pragma: no cover - network best effort\n        return\n    try:\n        installed_version = metadata.version(package_name)\n    except metadata.PackageNotFoundError:\n        return\n    if _normalise_version(latest_version) > _normalise_version(installed_version):\n        _emit(\n            f"Update available for {package_name}: installed {installed_version}, latest {latest_version}",\n            Fore.CYAN,\n        )\n\n\ndef ensure_dependencies() -> None:\n    for package, minimum_version in DEPENDENCIES.items():\n        _ensure_dependency(package, minimum_version)\n\n\n@dataclass\nclass NodeStatus:\n    name: str\n    version: str\n    status: str\n    colour: str\n    symbol: str\n\n\nSTATUS_COLOURS = {\n    "ok": (Fore.GREEN, "✔"),\n    "error": (Fore.RED, "✘"),\n}\n\n\ndef _render_status_table(status_rows: List[NodeStatus]) -> None:\n    width = 72\n    header = f" {TOOLKIT_NAME} v{TOOLKIT_VERSION} "\n    header_line = header.center(width, "=")\n    _emit(header_line, Fore.LIGHTMAGENTA_EX)\n    for row in status_rows:\n        colour = row.colour\n        symbol = row.symbol\n        payload = f"| {row.name:<30} v{row.version:<8} :: {row.status:<35} {symbol}"\n        _emit(payload.ljust(width), colour)\n    footer = "=" * width\n    _emit(footer, Fore.LIGHTBLUE_EX)\n\n\nensure_dependencies()\n\nstatus_log: List[NodeStatus] = []\n\ntry:\n    from .nodes import (  # noqa: F401\n        DEBUG_NODE_VERSION,\n        NODE_CLASS_MAPPINGS,\n        NODE_DISPLAY_NAME_MAPPINGS,\n        NODE_TOOLTIP_MAPPINGS,\n        PLOT_NODE_VERSION,\n        DebugATron3000,\n        PlotNode,\n    )\n    ok_colour, ok_symbol = STATUS_COLOURS["ok"]\n    status_log.append(NodeStatus("h4_Plot", PLOT_NODE_VERSION, "Loaded", ok_colour, ok_symbol))\n    status_log.append(NodeStatus("Debug-a-tron-3000", DEBUG_NODE_VERSION, "Loaded", ok_colour, ok_symbol))\nexcept Exception as exc:  # pragma: no cover - import failures should not kill startup\n    err_colour, err_symbol = STATUS_COLOURS["error"]\n    status_log.append(NodeStatus("h4_Plot", "-", f"Failed: {exc}", err_colour, err_symbol))\n    NODE_CLASS_MAPPINGS = {}\n    NODE_DISPLAY_NAME_MAPPINGS = {}\n\n_render_status_table(status_log)\n\nWEB_DIRECTORY = "js"\n\n__all__ = [\n    "PlotNode",\n    "DebugATron3000",\n    "NODE_CLASS_MAPPINGS",\n    "NODE_DISPLAY_NAME_MAPPINGS",\n    "NODE_TOOLTIP_MAPPINGS",\n    "WEB_DIRECTORY",\n]\n`;

    const nodesPre = document.getElementById('nodes-code');
    const initPre = document.getElementById('init-code');
    if (nodesPre) {
      nodesPre.textContent = nodesSource;
    }
    if (initPre) {
      initPre.textContent = initSource;
    }
  </script>
</body>
</html>
